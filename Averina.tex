\documentclass{csmathnotes}

\udc{214.16}
\title{Задача извлечения именованных сущностей в русскоязычном тексте}

\author{Аверина Мария Дмитриевна}
\position{магистр}
\affiliation{Ярославский государственный университет им. П.\,Г. Демидова}
\email{maverina518@gmail.com}

%\author{Дунаева Ольга Александровна}
%\position{к.ф.-м.н.}
%\affiliation{Ярославский государственный университет им. П.\,Г. Демидова}
%\email{olaydy@gmail.com}


\usepackage{diagbox}
\addbibresource{Averina.bib}


\begin{document}

\maketitle

\begin{abstract}
В статье рассматривается задача извлечения именованных сущностей из текста (\emph{NER}).
Для решения данной задачи часто используется метод \emph{CRF (conditional random fields)} --- условные случайные поля.
В статье исследуется вопрос решения задачи NER для текста на русском языке на основе метода \emph{CRF}.
При этом были использованы различные подходы к признаковому описанию текста, 
и был проведен сравнительный анализ моделей при использовании различных признаков и методов оптимизации.
Лучшая модель показала качество \emph{F1} равное $75\%-99\%$ в зависимости от сущности.
\end{abstract}

\keywords{распознавание именованных сущностей, \emph{NER}, условные случайные поля, \emph{CRF}, токенизация, нормализация слов, \emph{word2vec}, \emph{fastText}, \emph{F1}}.

\section*{Введение}
Одна из важнейших задач – сбор и анализ статистических данных нормативных
документов является достаточно трудоемкой для специалистов. На данный момент, в условиях повсеместного внедрения электронного документооборота, данная задача особенно актуальна. Автоматизация процесса анализа текстов – задача распознавания именованных сущностей~(\emph{named entity recognition}, \emph{NER})~\cite{base} позволит оптимизировать работу многих специалистов как по временным, так и по качественным показателям.


\section*{Задача извлечения сущностей}

Задача \emph{NER}~(\emph{named entity recognition}) заключается в выделении определенных непрерывных фрагментов текста~(сущности в тексте). Например, имеется новостной текст, в котором необходимо выделить некоторый заранее зафиксированный набор сущностей~(персоны, локации, организации, даты и т.д.). Таким образом требуется определить, что участок текста «1 января 1997 года» является датой, «Кофи Аннан» – персоной, а «ООН» – организацией~(рис. \ref{fig:ner}).


\begin{figure}[h]
	\center{\includegraphics[scale=0.5]{1.pdf}}
	\caption{Результаты работы NER.}
	\label{fig:ner}
\end{figure}


При разработке системы распознавания сущностей в качестве тестовых данных была использована выборка из $500$ файлов открытой русскоязычной базы судебной статистики~\cite{CourtsData}. Такой выбор был сделан в силу большого количества доступных документов, а также соответствующего задаче характера текстов~(наличие множества различных имён, дат, наименований, сумм и т.д.)

Для данной задачи была проделана разметка на инструменте \emph{BRAT}. \emph{BRAT (brat rapid annotation tool)} — онлайн-инструмент для разметки письменных текстов. Были выделены следующие сущности: номер документа(\emph{doc num}), суд(\emph{court}), судья(\emph{judge}), дата суда(\emph{date court}), истец(\emph{plaintiff}), ответчик или представитель(\emph{defendant}), решение суда(\emph{court decision}), статья или тип штрафа(\emph{payment fine}), сумма выплаты(\emph{payment amount}), срок обжалования(\emph{appeal time}).


\section*{Предварительная обработка данных}
Первым этапом является предобработка данных, которая включает в себя разбиение на слова, удаление ненужных символов, извлечение признаков слов. 
Токенизация — процесс разбиения текстового документа на отдельные слова, которые называются токенами. Для начала, весь текст необходимо токенизировать~\cite{Ner}, при этом удалить не несущие смысл символы и пробелы между цифрами. Далее весь текст приводится к нижнему регистру, а наличие заглавной буквы заносится в «словарь символов»(см. ниже).

«Словарь символов» - структура данных, которая хранит в себе информацию о находящихся рядом знаках препинания и о регистре слова. Например почта \emph{“v1@mail.com,”} будет разбита на \emph{“v1@”} и \emph{“mail.com,”}, где @ идентифицируется как слово-спецсимвол, а запятая заносится в «словарь символов». Символ “@” , позволяет классифицировать строку как почтовый адрес. Используемые нами спецсимволы: @, \#, № ,\%, $/$.


Приведем список признаков в «словаре символов»:
\begin{itemize}
    \item первая буква большая, остальные маленькие,
    \item все буквы маленькие,
    \item все буквы большие,
    \item первая буква маленькая,
    \item наличие запятой или точки в конце или начале слова и т.д.
\end{itemize}


По аналогии с ранее упомянутыми признаками "само слово" тоже можно использовать в качестве признака. Стоит отметить, что данный признак не является релевантным, если в обучающей выборке часто упоминается одна и та же фамилия или одно и тоже название организации, поскольку классификатор «заучивается» на определенное слово. Например, когда фамилия судьи одна и та же.


Большинство слов в тексте имеет падеж или склонение, что в свою очередь усложняет работу классификатора. Одим из способов решения данной проблемы является нормализация. Нормализация - приведение слов к «исходному» виду( лосями $\rightarrow$  лось, мыла $\rightarrow$ мыть, зеленого $\rightarrow$ зеленый). После нее число и род  можно отнести к отдельным признакам. Также возможно использовать часть речи(существительное, предлог и т.д.), как признак для распознавания сущностей. В случае спецсимволов, необходимо задавать каждому такому символу уникальные значения частей речи. Например, для символа \% морфологией будет \emph{PERCENT}.

 
Слова можно представлять в векторном пространстве. Процесс конвертации текста в векторы называется векторизацией~\cite{w2v}. Теперь после предобработки текста, нужно представить его в числовом виде, то есть закодировать текстовые данные числами, которые в дальнейшем могут использоваться в обучении. Технология \emph{Word2Vec} работает с большими текстовыми данными и по определенным правилам присваивает каждому слову уникальный набор чисел — семантический вектор. В последнее время все более популярным становится подход к векторизации текста, при котором \emph{Word2Vec} дополняется различными улучшениями. Два наиболее часто используемых улучшения — это \emph{GloVe} и \emph{fastText}. \emph{FastText} исправляет недостаток \emph{Word2Vec}: если обучение модели начинается с прямого кодирования одного \emph{D-мерного} вектора, то игнорируется внутренняя структура слов. Вместо прямого кодирования слов, \emph{fastText} предлагает изучать \emph{N-граммы} символов и представлять слова как сумму векторов \emph{N-грамм}.


Для улучшения качетсва обучения, можно учитывать не только признаки текущего токена, но и соседних токенов.

В дальнейшем будем использовать обозначения, где "список символов" - \emph{r}, слово - \emph{v}, часть речи - \emph{m}, нормализация - \emph{n}, \emph{Word2Vec} - \emph{w}, \emph{FastText} - \emph{f}, а цифра после буквы будет обозначать количество соседей влево и вправо(например, \emph{f3} это \emph{FastText} текущего слова и соседей по $3$ в лево и право).

\section*{Предсказание тегов при помощи CRF}
Наиболее  популярный инструмент для классификации именованных сущностей является \emph{CRF} (\emph{conditional random fields})~\cite{HabrCRF}. \emph{CRF} оптимизирует всю цепочку меток целиком, а не каждый элемент в этой цепочке.  Линейный \emph{CRF} хорошо подходит для решения задач сегментации и разметки последовательности, например: автоматическое выделение ключевых слов из текстов, выделение именованных сущностей (классификация сущностей), анализ тональности, автоматическое распознавание речи. \emph{CRF} может учитывать любые особенности и взаимозависимости в исходных данных. Так же \emph{CRF} хорошо работает в связке с рекуррентными нейросетями, моделирует совместное распределение на всей последовательности выходов сети одновременно.


Процесс обучения имеет большую вычислительную сложность, а именно $O(mNTQ2nS)$ где:
\begin{itemize}
    \item \emph{m} — количество тренировочных итераций,
    \item \emph{N} — количество обучающих последовательностей (из обучающей коллекции),
    \item \emph{T} — средняя длина обучающей последовательности,
    \item \emph{Q} — количество выходных классов,
    \item \emph{n} – количество признаков в обучающей матрице,
    \item \emph{S} – время работы алгоритма оптимизации на каждом шаге. 
\end{itemize}


На практике вычислительная сложность обучения~(время обучения) \emph{CRF} даже выше за счет всевозможных дополнительных операций таких, как сглаживание, преобразование данных из формата в формат и т.д. Отметим, что при увеличении количества признаков (например, за счет соседних слов) время обучения значительно увеличится. 

\section*{Метрика F1}
После того как \emph{CRF} обучен необходимо оценить качество его работы. Например, метрики \emph{Precision(P}) и \emph{Recall(R)} дают исчерпывающую характеристику классификатора. Но, как правило, при построении классификаторов приходится все время балансировать между двумя этими метриками. Если повысить \emph{Recall}, делая классификатор более «оптимистичным», это приводит к падению Precision из-за увеличения числа ложно-положительных ответов. Если же наоборот классификатор делать более «пессимистичным», то при росте \emph{Precision} это вызовет одновременное падение \emph{Recall} из-за отбраковки какого-то числа правильных ответов. Поэтому удобно для характеристики классификатора использовать одну величину, так называемую метрику \emph{F1}(среднегармоническая между \emph{Recall} и \emph{Precision})~\cite{А1}:

\begin{equation}
F1 = 2\frac{P R}{P + R} 
\end{equation}

Для оценки результатов было решено использовать \emph{F1}, но \emph{precision} и \emph{recall} будем считать, объединяя соседние слова с одним тегом в одну тегированную сущность. Поскольку одна сущность может состоять из нескольких токенов, то стоит при оценке качества учитывать ее целиком. 

\section*{Результаты}
Рассмотрим результаты тестирования на данных судебных протоколов: наилучшего качества классификации можно добиться за счет подбора параметров классификатора. Необходимо выбрать наиболее эффективный и быстрый оптимизатор для \emph{CRF}. В таблице ~\ref{tabl:table1} приведен анализ оптимизаторов, при фиксированном наборе признаков.


\emph{CRF} может учитывать любые особенности и взаимозависимости в исходных данных.
\begin{itemize}
	\item \emph{lbfgs} - градиентный спуск с использованием метода 
	\emph{L-BFGS},
	\item \emph{l2sgd} - стохастический  градиентный спуск с регуляризации \emph{L2},
	\item \emph{pa} - усредненный персептрон,
	\item \emph{ap} - passive аggressive,
	\item \emph{arow} - адаптивная регуляризация.
\end{itemize}

\begin{table}[!h]
    \begin{center}
        \begin{tabular}{|p{2cm}|p{1.6cm}|p{1.5cm}|p{1.5cm}|p{2.5cm}|}
            \hline
            Алгоритм оптимизации &  Долгое время работы & \emph{F1} на лучшей сущности(\emph{best}) & \emph{F1} на худшей сущности(\emph{worst}) & Примечания \\
            \hline
            \emph{lbfgs} & + & $1$ & $0.7$ & Время обучения более $5$ часов.  \\
            \hline
            \emph{l2sgd} & + & $1$  & $0$ & Классифицирует все, как самый. объемный класс \\
            \hline
            \emph{pa} & - & $1$  & $0.45$ & Время обучения не более $30$ минут. \\
            \hline
            \emph{ap} & - & $0.83$ & $0.24$  & - \\
            \hline
            \emph{arow} & - & $0.79$ & $0.31$  & - \\
            \hline
        \end{tabular}
    \end{center}
    \caption{\label{tabl:table1}Сравнительный анализ алгоритмов оптимизации.}
\end{table}

Как видно из таблицы ~\ref{tabl:table1} лучший результат показали алгоритмы оптимизации \emph{pa} и \emph{lbfgs}. Стоит отметить, что \emph{lbfgs}  более трудоемкий по сравнению с \emph{pa}. В дальнейших тестах будем использовать эти два оптимизатора.


\begin{table}[!h]
    \begin{center}
        \begin{tabular}{|p{2.1cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}
            \hline
            \diagbox[width=7.2em]{Признаки}{Алгоритм} &  \emph{pa} & \emph{l2sgd} & \emph{lbfgs} \\
            \hline
            $(r3, v3)$ & - & - & 
            best:\newline  doc num  $0.993$ \newline
            worst:\newline defendant  $0.751$ \newline
            time fit:\newline  $5.55.36$ \\
            \hline
            $(r3, v1)$ & best: \newline judge $0.906$ \newline
            worst: \newline defendant   $0.455$ 
            \newline time fit:  \newline $0.07.58$
            & best:\newline judge  $0.867$ \newline
              worst: \newline defendant    $0.116$ \newline
              time fit:\newline $0.4.11$
            & best:\newline   doc num  $0.973$ \newline
              worst:\newline defendant  $0.685$\newline
              time fit:\newline  $3.32.49$ \\
            \hline
            $(w1, v1, m1)$ 
            & best:\newline    court   $0.827$ \newline
            worst: \newline defendant   $0.331$ \newline
            time fit:\newline $0.05.20$ \newline
            & best: \newline judge    $0.751$ \newline
              worst: \newline court  $0.013$ \newline
              time fit:\newline $0.3.14$
            & best:\newline  judge $0.971$\newline
              worst: \newline defendant  $0.541$\newline
              time fit:\newline $3.03.01$\\
            \hline
            $(r3, m3)$
            & best: \newline judge $0.692$ \newline
            worst: \newline payment $0.412$ \newline
            time fit: \newline  $0.33.58$
            & best:\newline   date court   $0.746$ \newline
             worst: \newline court $0.126$ \newline
             time fit: \newline $0.12.11$
            & best: \newline   judge $0.818$ \newline
              worst:\newline defendant $0.285$ \newline 
              time fit: \newline $3.58.24$\\
            \hline
        \end{tabular}
    \end{center}
    \caption{\label{tabl:table2}Сравнительный анализ \emph{F1}.}
\end{table}

Заметим, что наименьший разброс \emph{F1}  принимает при признаках $(r3,v3)$ с оптимизатором \emph{lbfgs}, при этом демонстрируя достаточно высокий средний результат(таблица ~\ref{tabl:table2}). Однако, время обучения такой модели более пяти часов. При необходимости уменьшить время обучения наиболее оптимальным решением будет обучение модели на признаках $(r3,v1)$ с оптимизатором \emph{pa}.

\section*{Заключение}
Подведем итоги. Нами была решена задача морфологической классификации. В данной статье проведен сравнительный анализ алгоритмов оптимизации, различных наборов признаков. Алгоритм \emph{pa} с признаками $(r3, v1)$ показал наилучший результат по времени и \emph{F1}  в совокупности. Алгоритм \emph{lbfgs}  с признаками $(r3, v3)$ продемонстрировал лучший результат по \emph{F1}, но оказался трудоемким и долгообучаемым. Стоит отметить, что лучший результат показал алгоритм \emph{pa} по времени. Но для более точного предсказания будет необходимо использовать \emph{lbfgs}.


Для дальнейшего улучшения результатов планируется выделять еще новые признаки, а также можно попробовать использовать другие методы классификации и усовершенствование архитектуры классификатора на основе \emph{biLSTM} и \emph{BERT}

\printbibliography

\end{document}
